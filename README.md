# Movement Chain AI - Machine Learning

ML models and training pipelines for real-time movement analysis, error detection, and corrective feedback in the Movement Chain AI system.

## Overview

This repository contains:

- **Pose estimation models** (RTMPose-m, YOLO11 Pose)
- **Temporal modeling** (LSTM + Transformer hybrid for movement sequences)
- **Error detection algorithms** (kinematic, timing, muscle pattern analysis)
- **Model optimization** (ONNX conversion for cross-platform deployment)
- **Training pipelines** (data preprocessing, augmentation, evaluation)

## Architecture

See [full documentation](https://movement-chain-ai.github.io/system-documentation/) for system architecture.

### Model Pipeline

```
Camera Input (60fps) ‚Üí RTMPose-m ‚Üí 17 keypoints (34D)
IMU Data (100Hz) ‚Üí Feature Extraction ‚Üí 6D vector
EMG Data (200Hz) ‚Üí Signal Processing ‚Üí 4D vector
Metadata ‚Üí User Profile + Context ‚Üí 7D vector
                    ‚Üì
        Feature Fusion ‚Üí 51D input vector
                    ‚Üì
    LSTM + Transformer ‚Üí Movement sequence analysis
                    ‚Üì
    Error Detection ‚Üí Correction suggestions
```

## Planned Directory Structure

```
movement-chain-ml/
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ golf_swing/              # Golf movement datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bicep_curl/              # Workout datasets (MVP)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_loaders.py          # PyTorch data loading
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pose_estimation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rtmpose_m.py         # RTMPose-m architecture
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ yolo11_pose.py       # YOLO11 Pose alternative
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ temporal/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm_transformer.py  # Hybrid temporal model
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ attention.py         # Custom attention layers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ error_detection/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ kinematic_analyzer.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ form_classifier.py
‚îÇ   ‚îú‚îÄ‚îÄ train.py                     # Training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                  # Model evaluation
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ golf_config.yaml         # Golf-specific config
‚îÇ       ‚îî‚îÄ‚îÄ workout_config.yaml      # Workout config
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ onnx_export.py               # ONNX model conversion
‚îÇ   ‚îú‚îÄ‚îÄ tflite_export.py             # TensorFlow Lite export
‚îÇ   ‚îú‚îÄ‚îÄ quantization.py              # Model quantization (INT8)
‚îÇ   ‚îî‚îÄ‚îÄ benchmark.py                 # Performance benchmarking
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ rtmpose_m_golf.onnx         # Compiled models
‚îÇ   ‚îú‚îÄ‚îÄ lstm_transformer.onnx
‚îÇ   ‚îî‚îÄ‚îÄ error_detector.tflite
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ camera_calibration.py   # Camera intrinsics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ imu_filtering.py        # Sensor fusion
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ emg_normalization.py    # EMG signal processing
‚îÇ   ‚îú‚îÄ‚îÄ augmentation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pose_augment.py         # Keypoint augmentation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ temporal_augment.py     # Sequence augmentation
‚îÇ   ‚îî‚îÄ‚îÄ annotation/
‚îÇ       ‚îú‚îÄ‚îÄ label_studio_export.py  # Annotation tools
‚îÇ       ‚îî‚îÄ‚îÄ ground_truth.py         # Expert labels
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ exploratory_analysis.ipynb  # Data exploration
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.ipynb      # Benchmark results
‚îÇ   ‚îî‚îÄ‚îÄ error_patterns.ipynb        # Error taxonomy analysis
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py              # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_inference.py           # Inference tests
‚îÇ   ‚îî‚îÄ‚îÄ test_data_pipeline.py       # Data loading tests
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ setup.py                         # Package installation
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE                          # Apache 2.0
‚îî‚îÄ‚îÄ README.md
```

## Development Setup

### Prerequisites

- Python 3.10+
- CUDA 11.8+ (for GPU training)
- 16GB+ RAM (32GB recommended)

### Installation

```bash
# Clone repository
git clone https://github.com/movement-chain-ai/movement-chain-ml.git
cd movement-chain-ml

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

### Dependencies

```
# Core ML frameworks
torch>=2.0.0
onnx>=1.14.0
onnxruntime>=1.15.0
tensorflow-lite>=2.13.0

# Pose estimation
mmpose>=1.1.0
mmcv>=2.0.0
ultralytics>=8.0.0

# Data processing
numpy>=1.24.0
pandas>=2.0.0
opencv-python>=4.8.0
scikit-learn>=1.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0

# Development
pytest>=7.4.0
black>=23.7.0
flake8>=6.0.0
```

## Model Zoo

### Pose Estimation Models

| Model | Input | Output | FPS (Intel i7) | AP | Status |
|-------|-------|--------|----------------|-----|--------|
| RTMPose-m | 256x192 | 17 keypoints | 90+ | 75.8% | ‚úÖ Recommended |
| YOLO11 Pose | 640x640 | 17 keypoints | 60+ | 89.4% | ‚ö†Ô∏è Alternative |

### Temporal Models

| Model | Input | Output | Latency | Accuracy | Status |
|-------|-------|--------|---------|----------|--------|
| LSTM + Transformer | 51D √ó 30 frames | Error probabilities | 15ms | TBD | üöß In Progress |

### Deployment Formats

- **ONNX Runtime** (Mobile app inference)
- **TensorFlow Lite** (Mobile GPU acceleration)
- **TFLite Micro** (ESP32-S3 edge inference - future)

## Training Workflow

```bash
# 1. Prepare dataset
python data/preprocessing/prepare_dataset.py --movement golf_swing

# 2. Train pose estimation model
python training/train.py --config config/golf_config.yaml --model rtmpose_m

# 3. Train temporal model
python training/train.py --config config/golf_config.yaml --model lstm_transformer

# 4. Export to ONNX
python inference/onnx_export.py --checkpoint checkpoints/best_model.pth

# 5. Benchmark performance
python inference/benchmark.py --model models/rtmpose_m_golf.onnx
```

## Model Deployment

### ONNX Runtime (Mobile App)

```python
import onnxruntime as ort

session = ort.InferenceSession("models/rtmpose_m_golf.onnx")
outputs = session.run(None, {"input": camera_frame})
keypoints = outputs[0]
```

### TensorFlow Lite (Flutter)

```dart
import 'package:tflite_flutter/tflite_flutter.dart';

final interpreter = await Interpreter.fromAsset('rtmpose_m_golf.tflite');
interpreter.run(inputTensor, outputTensor);
```

## Performance Targets

- **Pose Estimation:** <30ms latency on mobile GPU
- **Temporal Analysis:** <50ms for 30-frame window
- **End-to-End:** <100ms total feedback latency
- **Accuracy:** >85% error detection rate

See [performance targets documentation](https://movement-chain-ai.github.io/system-documentation/latest/architecture/hld/04-performance-targets/).

## Contributing

We welcome contributions! Submit pull requests with:
- ‚úÖ Unit tests for new models
- ‚úÖ Benchmark results comparison
- ‚úÖ Documentation updates

Branch protection requires:
- At least 1 approving review
- All CI checks passing

## License

Apache License 2.0 - see [LICENSE](LICENSE) file for details.

## Documentation

Full system documentation: https://movement-chain-ai.github.io/system-documentation/

## Related Repositories

- [movement-chain-mobile](https://github.com/movement-chain-ai/movement-chain-mobile) - Flutter app (model deployment)
- [movement-chain-firmware](https://github.com/movement-chain-ai/movement-chain-firmware) - ESP32 firmware
- [movement-chain-hardware](https://github.com/movement-chain-ai/movement-chain-hardware) - Hardware schematics
